{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first glimpse into Neural Networks: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "In this first notebook, you will learn the basics of MLP with Keras. Before we can jump into Neural Networks, we need to take a look at the dataset, visualize it, analyze it, and prepare it so that it can be used to train a simple neural network.\n",
    "\n",
    "## 1. Setting up the environment\n",
    "The first code block simply imports packages we need to obtain, preprocess, and analyze the data. We import\n",
    "\n",
    "- [NumPy](https://numpy.org/): standard numerical library, used for linear algebra, vector, matrix, and tensor manipulation.\n",
    "- [Matplotlib](https://matplotlib.org/): 2D plotting library with a MATLAB-like interface.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/): Machine Learning library, for standard ML methods such as regressions, Support Vector Machines, Random Forests, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# In order to ignore FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The data: Iris dataset\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set):\n",
    "\n",
    ">The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the GaspÃ© Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "\n",
    ">The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "We will load the iris dataset from scikit-learn (we imported the function in the code block above). The dataset is organized like a [Python dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries), you can access the different features, if you know what they are called. Let us explore the dataset: we first load it, then print out the dictionary keys. Eventually, we print out the description, which we know is stored as `DESCR`. Some numbers in the description will be useful below, when we will check that the data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "print(iris.keys())\n",
    "\n",
    "# Print the dataset description:\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to extract the information we need, that is, assign parts of the dataset to our variables. Why doing so? At least for two reasons:\n",
    "\n",
    "- Simplicity: we do not want to always reference a field of the `iris` variable.\n",
    "- Consistency: we will often re-use the same names in our DL pipelines, but in the original datasets, different namings could have been chosen.\n",
    "\n",
    "Thus, we want to have in a variable called `X` the real input *data* we are going to classify, in this case, for each sample, we will have the four attributes listed in the above description (`X` will thus be a matrix, every sample will be represented by a line, every column will represent a feature). `y` will always represent our ground truth (at least for labeled data): it is a vector assigning the correct label index to each sample.\n",
    "\n",
    "We will also use two variables `names` and `feature_names`, to store the class name corresponding to the label indices used in `y` and the feature name corresponding to each column of `X`, respectively.\n",
    "\n",
    "You only have to fill the matrix `X` and the vector `y`, in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: assign the correct quantities to X and y\n",
    "X = \n",
    "y = \n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember from the lecture, our NN will need a so-called one-hot-encoded vector representing the label of each sample. A one-hot-encoded vector is a vector where all components are `0` except for the component corresponding to the correct sample's label, which is set to `1`. There are many ways to turn `y` into the one-hot-encoded matrix `Y` (where each line will represent the one-hot-encoding of a sample's label. In the next code block, we use an object of type `OneHotEncoder` from `sklearn.preprocessing` which we imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for a sanity check! We have two matrices, `X` and `Y`, can you predict what their shapes will be like? (hint: read the above description again if you can't!) \n",
    "\n",
    "We will call `n_features` the number of features measured for each sample, `n_samples` the number of samples, and `n_classes` the number of classes represented in our dataset. You should extract these numbers from `X` and `Y` (every NumPy array has an attribute called `shape`...) and check that they are in agreement with your prediction. Then, you should also make sure that `X` and `Y` have one dimension which is the same... can you guess which one, and introduce a check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign the correct values to the next three variables!\n",
    "n_features =\n",
    "n_classes = \n",
    "n_samples = \n",
    "\n",
    "print(\"Number of features: {}\".format(n_features))\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Number of samples: {}\".format(n_samples))\n",
    "\n",
    "# TODO: insert a check to ensure X and Y refer to the same number of samples\n",
    "#       you can use a soft check (printing a warning if something is wrong)\n",
    "#       or a hard check (using an assert of raising an error).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing the data\n",
    "We now have some data, but NNs are a little picky about numbers. To have good convergence, all data should (at least) be scaled to have zero mean, and a variance of `1`. Fortunately, an object of type `StandardScaler` will make things easy for us.\n",
    "\n",
    "We also want to split our (small) dataset equally into training and test portions. We use the built-in function `train_test_split` and are ready to visualize the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the data\n",
    "The next code block uses `matplotlib` to give a visual feedback about the dataset. The desired result is simple: two plots, in the first one, input feature `1` is plotted agains input feature `0`, in the second one, input feature `3` is plotted agains input feature `2`.\n",
    "\n",
    "The code takes care of separating the different classes when plotting, all you have to do is to fill in three annotations for each plot:\n",
    "\n",
    "- class name as label of each set of points (replace \"set 0\", \"set 1\", and \"set 2\")\n",
    "- feature name for x-axis label\n",
    "- feature name for y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data sets\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    #TODO replace this label to match the class names!\n",
    "    label_string = \"set_{}\".format(target)\n",
    "    plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=label_string)\n",
    "    \n",
    "#TODO replace this label with the correct x-axis feature name!\n",
    "plt.xlabel(\"x label\")\n",
    "#TODO replace this label with the correct y-axis feature name!\n",
    "plt.ylabel(\"y label\")\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    label_sting = target_name # \"set_{}\".format(target)\n",
    "    plt.plot(X_plot[:, 2], X_plot[:, 3], linestyle='none', marker='o', label=target_name)\n",
    "    \n",
    "#TODO replace this label with the correct x-axis feature name!\n",
    "plt.xlabel(\"x label\")\n",
    "#TODO replace this label with the correct y-axis feature name!\n",
    "plt.ylabel(\"y label\")\n",
    "plt.axis('equal')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying Neural Networks to the problem\n",
    "\n",
    "But... weren't we supposed to try out neural networks? Of course! But with the right toolkits, neural networks are easy to set up and run, once you have the data! We will now start with a simple (very simple) 2-layer perceptron!\n",
    "\n",
    "In [Keras](https://keras.io), fully connected layers are represented by object of `Dense` type. An MLP is a simple network, thus we will build it as a `Sequential` model, and we only need to import two classes from the correct files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, time to create the Neural Network! We will call this NN `first_model`, and name it `iris_MLP`. We give you the input and the output layers, you will just have to:\n",
    "\n",
    "- assign the correct value to the `nn_input_dim` variable, representing the size of *one* input sample (Keras will take care of adjusting the pipeline for the batch size)\n",
    "- assign the correct value to the `nn_output_dim` variable, representing the size of *one* prediction (or output)\n",
    "- create some hidden layers: use as many layers as you like, but don't overdo! Remember this is a fairly simple problem! You can even skip the hidden layers, if you like! Remember, for a `Sequential` model, you do not need to define input size for hidden layers, as it is automatically determined by Keras. You only specify the output size as `units`.\n",
    "\n",
    "Notice that we are using `'relu'` as activation function. Feel free to explore other possibilities!\n",
    "\n",
    "Once you defined your model (thus, after adding the output layer), you need to call `compile`, so that Keras can build the correct pipelines (feedforward and backpropagation) to train and use your model in inference mode. As we are dealing with labels, we use `'categorical_crossentropy'` as loss function. Then, we use a standard version of the Adam optimizer, and as metric we only want to see the classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assign the right values to input and output sizes\n",
    "nn_input_dim = \n",
    "nn_output_dim = \n",
    "\n",
    "# Create model\n",
    "first_model = Sequential(name=\"iris_MLP\")\n",
    "\n",
    "# You can use this size for all layers, or change it according to their position...\n",
    "first_layer_output_size=8\n",
    "first_model.add(Dense(units=first_layer_output_size, input_dim=nn_input_dim, activation='relu'))\n",
    "\n",
    "#TODO add as many hidden layers as you wish\n",
    "\n",
    "\n",
    "first_model.add(Dense(nn_output_dim, activation='softmax'))\n",
    "\n",
    "# Compile model: since we want to use the Adam optimizer with standard parameters\n",
    "#                we do not instantiate a keras.models.optimizers.Adam object, instead\n",
    "#                we only call it by its string name\n",
    "first_model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print a nice description of the model!\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to be able to train multiple models and compare their results, we create an array holding models and start by populating it with `first_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [first_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you can define as many models as you like. You could even create them in a loop, modifying something at each iteration (number of nodes? layers? both?). Just remember to always compile them and `append` them to `models`! We start by adding a simple model with no hidden layers, but feel free to modify it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = Sequential(name=\"second\")\n",
    "first_layer_output_size=8\n",
    "second_model.add(Dense(units=first_layer_output_size, input_dim=nn_input_dim, activation='relu'))\n",
    "\n",
    "#TODO add as many hidden layers as you wish\n",
    "\n",
    "second_model.add(Dense(nn_output_dim, activation='softmax'))\n",
    "\n",
    "second_model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "models.append(second_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to store the training and validation accuracy (and loss) we get at training time. To do this, we create a Python dictionary linking each model to its training history (the `history_callback`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the trainings! For each model we added to `models` we call the `fit` function. Fill the call with the right `x` and `y` variables (these should be the variables you want to train your model on). The rest of the call has already been filled for you (thank us later).\n",
    "\n",
    "After the model has been trained, we compute and print out its accuracy on the test data.\n",
    "\n",
    "For each training, check the output: we set `verbose=1`, so that Keras prints out updates after each batch. It reports loss and accuracy for both training and validation datasets. Compare the last value of `val_acc` and the value of `score[1]`, which we print as `Test accuracy`. What do you observe? Can you explain this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 50\n",
    "\n",
    "for model in models:\n",
    "    print('Model name:', model.name)\n",
    "    #TODO assign the right variables to x and y\n",
    "    history_callback = model.fit(x=, y=,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=1,\n",
    "                                 validation_data=(X_test, Y_test))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('\\n')\n",
    "\n",
    "    history_dict[model.name] = [history_callback, model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some more plotting! From the histories we stored, we access `'acc'`, `'loss'`, '`val_acc`', and `'val_loss'`. We create two plots, one for accuracy and one for loss, then we plot the learning curves of each model. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n",
    "\n",
    "colorstring = \"rgbcmk\"\n",
    "\n",
    "for i,model_name in enumerate(history_dict):\n",
    "    train_acc = history_dict[model_name][0].history['acc']\n",
    "    train_loss = history_dict[model_name][0].history['loss']\n",
    "    val_acc = history_dict[model_name][0].history['val_acc']\n",
    "    val_loss = history_dict[model_name][0].history['val_loss']\n",
    "    ax1.plot(train_acc, colorstring[i]+':', label=model_name+\" train\")\n",
    "    ax1.plot(val_acc, colorstring[i], label=model_name+\" val\")\n",
    "    ax2.plot(val_loss, colorstring[i], label=model_name)\n",
    "    \n",
    "ax1.set_ylabel('accuracy')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you just built and trained your first Neural Networks! Now take some time to do some quick experiment, add more layers, more nodes, change the optimizer... [Keras docs](https://keras.io/) are your friends! What is the best accuracy you can get? How quick? You can also change batch size and number of epochs you want to train for... have fun! (Hint: you can suppress the training output by setting `verbose` to `0`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
