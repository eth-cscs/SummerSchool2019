{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: a classic real-world use case\n",
    "\n",
    "In this notebook, you will apply a Multi-Layer Perceptron to the famous MNIST dataset. You will have a chance to play around with different training techniques, and see how they influence the results your MLP will obtain.\n",
    "\n",
    "## 1. Setting up the environment\n",
    "As usual, the first step is to load all needed libraries. We are using the MNIST dataset, which is automatically downloaded by Keras when needed, through the `keras.datasets` module. We will not need `scikit-learn`, and we will find pre-processing routines in Keras and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The data: MNIST dataset\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database):\n",
    "\n",
    ">The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n",
    "\n",
    ">The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. \n",
    "\n",
    "In Keras, we can load the MNIST dataset using a built-in function, which will automatically load both training and test dataset. The one-hot-encoded versions of `y_train_original` and `y_test` are obtained with `keras.utils.to_categorical`. You need to define `n_classes`, i.e. the number of labels we are classifying our samples into. Since we are dealing with hand-written digits, you might guess what value you have to assign to `n_classes`, otherwise, you can explore `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(X_train_original, y_train_original), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# TODO assign the right value to num_classes, i.e. the number of different labels of our samples\n",
    "n_classes = \n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = keras.utils.to_categorical(y_train_original, n_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need to extract some useful information from the data and check that the datasets have the shape we expect. We are interested in four values:\n",
    "\n",
    "- `n_train_samples`: the number of samples in our training set\n",
    "- `img_rows`: the number of rows in one image sample\n",
    "- `img_cols`: the number of columns in one image sample\n",
    "- `n_test_samples`: the number of samples in our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign the right values to the following variables\n",
    "n_train_samples = \n",
    "img_rows = \n",
    "img_cols = \n",
    "n_test_samples  = \n",
    "\n",
    "print(\"Training dataset: {} images, each one of size {}x{} pixels\".format(n_train_samples, img_rows, img_cols))\n",
    "print(\"Test dataset: {} images\".format(n_test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing the data\n",
    "MLPs only accept vectors (or, if they accept matrices, they treat them as vectors). Since our samples are 2D gray scale images, thus matrices, we need to reshape them. To do this, we will use the NumPy function `reshape`. Every image will be turned into a vector: its size must be equal to the number of pixels contained in one image. Can you assign the right value to `img_pixels`, in the code block below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assign the right value \n",
    "img_pixels = \n",
    "X_train = X_train_original.reshape(n_train_samples, img_pixels)\n",
    "X_test = X_test.reshape(n_test_samples, img_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to scale the data. In this case, the original images are given as 8-bit integers (`0`-`255`), thus we first cast them to floating point numbers, and then scale them to be in the `0`-`1` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing the data\n",
    "Now that you have prepared the data, why not look into it? \n",
    "\n",
    "Let us plot some samples! We suggest you use the function `plt.imshow` as shown in the following code block. Remember to use the samples stored in `x_train_original`, and not their vectorized counterparts (stored in `x_train`), as the latter do not look very interesting when plotted! Addd a title to the image you plot (with `plt.title(\"YOUR TITLE\")`), and use it to display the sample label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#TODO  assign a sample to this variables\n",
    "# (or put this into a loop and use it with subplot, as in the previous notebook, to see more samples at once)\n",
    "\n",
    "sample = \n",
    "sample_label = \n",
    "plt.imshow(sample,cmap='gray')\n",
    "plt.title(\"This image is a \" + str(sample_label) ); #The semicolon suppresses text output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying Neural Networks to the problem\n",
    "\n",
    "Alright! Time to use Neural Networks again! As usual, you decide how to shape the network. Some suggestions:\n",
    "\n",
    "- Input and output layers are already written for you, just assign the right values to input and output dimensions.\n",
    "- Dense layers should have a number of nodes similar (not necessarily equal) to the input size\n",
    "- Start with dense layers only. Use `'relu'` as activation function.\n",
    "\n",
    "After you've defined your network, you will have to `compile` the model (look at the previous notebook if you do not remember how!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, SGD, Adagrad, Adadelta, Adam\n",
    "\n",
    "nodes = 512\n",
    "\n",
    "#TODO assign input and output dimensions\n",
    "input_dimension = \n",
    "output_dimension = \n",
    "dropout_prob = 0.2\n",
    "\n",
    "model = Sequential()\n",
    "# Input layer\n",
    "model.add(Dense(nodes, activation='relu', input_dim=input_dimension))\n",
    "# TODO add hidden layers\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(output_dimension, activation='softmax'))\n",
    "\n",
    "model.name= \"MNIST_MLP\"\n",
    "model.summary()\n",
    "\n",
    "# TODO: compile the model. As metrics we use accuracy,\n",
    "#       as loss we use categorical_crossentropy\n",
    "#       as optimizer we start with SGD, which we simply\n",
    "#       instantiate with default parameters, by writing\n",
    "#       optimizer=SGD() or optimizer='sgd'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's training time! Define some easy hyper-parameters, such as `batch_size` and `epochs`. Then call `model.fit`... again, if you don't remember how, look at the previous notebook!\n",
    "\n",
    "Then, once the training is finished, we print the test score. Is it good? Does it satisfy you? We will look at some more details in the next blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# TODO: call the fit function, you have all needed parameters!\n",
    "history_callback = \n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, once you've trained a network, you might want to use it in _inference mode_.\n",
    "\n",
    "To do this, we can use `model.predict(sample)`: as input, it requires an array of samples, thus, if you only want to give it one sample (the $n$th sample), you will have to take a slice of an array, like `X_train[n:(n+1)]`. Can you interpret the output? And can you get the predicted label from it?\n",
    "\n",
    "Optional: find a sample for which the prediction is wrong and plot it. Why you think it was mis-classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# We pick sample 32, for no reason...\n",
    "array_slice = X_train[32:33]\n",
    "prediction = model.predict(array_slice)\n",
    "\n",
    "plt.imshow(X_train_original[32,:,:],cmap='gray')\n",
    "plt.title(\"This image is a \"+str(y_train_original[32]) + \" and is predicted as a \" +\n",
    "          str(np.argmax(prediction)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to see how the training effectively went. Use `history_callback` to look at training and validation accuracy. Try to answer the following questions:\n",
    "\n",
    "- Did the model over- or underfit the data?\n",
    "- Do you think that training for more epochs would help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: adapt the code from the previous notebook and plot accuracy and loss for training and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the previous code blocks and modify them, trying to achieve a better (or faster) result. Some suggestions:\n",
    "\n",
    "- how large can the batch size be? How does it influence the achieved accuracy?\n",
    "- how many epochs should you train for?\n",
    "- what optimizer should you use? We imported a bunch of different optimizers, but you can find more in the [_Optimizers_ section of the docs](https://keras.io/optimizers/)!\n",
    "- if your model overfits, you can either reduce the number of hidden layers, or add a dropout layer after each dense layer. It is as easy as writing `model.add(Dropout(dropout_prob))` after each dense layer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
