{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN: Deep Convolutional Generative Adversarial Network\n",
    "\n",
    "In this notebook, you will reproduce the results obtained in the famous paper [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "# 1. Setting up the environment\n",
    "We import here all packages you will need throughout this notebook. Notice that we will be using the `LeakyReLU` activation function for the discriminator, but since it is not part of the standard activations functions, we will find it in `keras.layers.advanced_activations`. We will use a smaller version of the network defined in the paper, because we aim at generating MNIST-like images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The data: MNIST dataset\n",
    "To train the discriminator, we need real images. We will use MNIST images (but you could also use the German Traffic Sign dataset, if you're feeling brave!). Notice that it is sufficient to use the training set images, thus we use the underscore symbol `_` to state that we ignore all other outputs of the function `mnist.load_data()`. As usual, fill the blank variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# TODO assign the correct values to the following variables!\n",
    "n_train_samples =\n",
    "img_rows =\n",
    "img_cols =\n",
    "# TODO this time, we also explicitly define the number of channels\n",
    "n_channels = \n",
    "\n",
    "print(\"Training dataset: {} images. Size {}x{} {}-channel pixels\".format(n_train_samples, img_rows, img_cols, n_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing the data\n",
    "It has been empirically found that it is better to scale the pixel values to the $[-1, 1]$ range. Apply the correct transformation to the `X_train` samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Rescale data to the [-1, 1] range. NOT to [0,1]!\n",
    "X_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the shape of the input of our discriminator network. This is the shape of one MNIST image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO assign the right value to input_shape\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = \n",
    "else:\n",
    "    input_shape = \n",
    "\n",
    "\n",
    "X_train = X_train.reshape(n_train_samples, input_shape[0], input_shape[1], input_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing the data\n",
    "OK, unless you are using a new dataset, you can skip this step!\n",
    "\n",
    "# 5. Applying Neural Networks to the problem\n",
    "Let us recap what the two neural networks will do:\n",
    "\n",
    "- the **generator** takes an array of random numbers as input and returns an image of the same size of those contained in the training set. The size of the random number array is called the size of the latent dimension or, in our code `latent_dim`: we set it to 100.\n",
    "- the **discriminator** takes an image as input and the probability of the image being real, with `0` (`1`) meaning the image is classified as fake (real) with absolute certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a function which we will use to build the discriminator. The discriminator is a simple CNN which takes an image as input and outputs a single number. Fill the missing variables, and notice that, since we are using an advanced activation function, we need to create a layer holding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO define this single missing variable!\n",
    "output_dim = \n",
    "\n",
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=input_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the generator. The generator starts from an array of size `latent_dim` and outputs an image of the correct size (equal to the size of the real images) as output. Notice that we use a `UpSampling2D` layer, which you might want to lookup in the [documentation](https://keras.io/layers/convolutional/#upsampling2d)!\n",
    "\n",
    "Compare this generator to the one described in the [original paper](https://arxiv.org/abs/1511.06434) (look at the box on page 3, and at image on page 4). What differences do you see? What do the two networks have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(n_channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the discriminator and we compile it. We use Adam solver, but not with standard parameters! You can find the right parameters in the original paper, or, more easily, in [this post...](https://medium.com/@jonathan_hui/gan-dcgan-deep-convolutional-generative-adversarial-networks-df855c438f)\n",
    "\n",
    "With respect to previous notebooks, there is a different loss function... can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assign the correct values to the following variables\n",
    "adam_learning_rate =\n",
    "momentum_beta1     = \n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(adam_learning_rate, momentum_beta1),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to instantiate the generator. After that, we create a stacked model, which puts generator and discriminator together. To do this, we use the `Model API`, instead of the `Sequential` API we have used until now.\n",
    "\n",
    "The stacked model has:\n",
    "\n",
    "- an input layer `z` which takes the random noise vector\n",
    "- the generator, which takes `z` as input and returns a generated image `img`\n",
    "- the discriminator, which takes `img` as input and outputs a probability `valid`. The discriminator will not be trained in this phase (we have a separate training for it, see below!)\n",
    "- `valid` will also be the output of our `combined` model. You can see that we build it by calling the Model constructor which takes two simple arguments, the input and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(inputs=z, outputs=valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(adam_learning_rate, momentum_beta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the networks!\n",
    "\n",
    "This training is different from the usual `fit` or `fit_generator` functions we were using in the previous notebooks. Here, we first train the discriminator on **one batch of real images**, then on **one batch of generated images**. After that, we train the stack model (where we do not train the generator part anymore) on a set of random vectors called `noise`. We then repeat these three steps as much as we need. Fill the only missing variable (again, look at [this post...](https://medium.com/@jonathan_hui/gan-dcgan-deep-convolutional-generative-adversarial-networks-df855c438f)), understand what the code is doing (especially where you find a *`TODO`*) and run it! You might be interested in reading how `train_on_batch` is defined, [here](https://keras.io/models/model/#train_on_batch)... and disregard any warning message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=4001\n",
    "save_interval=50\n",
    "#TODO set the correct batch size!\n",
    "batch_size=\n",
    "\n",
    "\n",
    "# TODO can you understand what these two variables are needed for?\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake  = np.zeros((batch_size, 1))\n",
    "\n",
    "\n",
    "for step in range(steps):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    # Sample noise and generate a batch of new images\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "    # This is just an indication of the average loss!\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Train the generator (wants discriminator to mistake images as real)\n",
    "    # TODO: why do we use valid?\n",
    "    g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if step % save_interval == 0:    \n",
    "        # Plot the progress\n",
    "        titlestring=\"Step {} [D loss: {}, acc.: {:.2f}%] [G loss: {}]\".format(step, d_loss[0], 100*d_loss[1], g_loss)\n",
    "        #print(titlestring)\n",
    "        noise = np.random.normal(0, 1, (20, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        plt.figure(figsize=(100,10))\n",
    "        for j in range(10):\n",
    "            plt.subplot(1, 10, j+1)\n",
    "            plt.imshow(gen_imgs[j, :,:,0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        #fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.suptitle(titlestring, fontsize=80)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
