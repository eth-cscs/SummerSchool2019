{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Euclidean Distance Matrix with NumPy\n",
    "\n",
    "In this notebook we implement a function to compute the Euclidean distance matrix. We use a simple algebra trick that makes possible to write the function in a completely vectorized way in terms of optimized NumPy functions. We  compare this function to a naive implementation that uses loops to combine all rows into the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_loops(x, y):\n",
    "    \"\"\"Euclidean square distance matrix\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N,) numpy array\n",
    "    y: (N,) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = x_ij^2 - y_ij^2\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    dist_matrix = np.zeros((num_samples, num_samples))\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            diff = xi - yj\n",
    "            dist_matrix[i][j] = np.dot(diff, diff)\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_numpy(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N,) numpy array\n",
    "    y: (N,) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = x_ij^2 - y_ij^2\n",
    "    \"\"\"\n",
    "\n",
    "    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "    y2 = np.einsum('ij,ij->i', y, y)[:, np.newaxis].T\n",
    "\n",
    "    xy = np.dot(x, y.T)\n",
    "\n",
    "    return np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `euclidean_numpy` function\n",
    "\n",
    "Each element of the Euclidean distance matrix is the scalar product of the diference between two rows fo the dataset. `euclidean_numpy` takes advantage of this by doing the following\n",
    "$$\\sum_k {(x_{ik}-y_{ik})^2} = (\\vec{x}_i - \\vec{y}_j)\\cdot(\\vec{x}_i - \\vec{y}_j) = \\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j - 2\\vec{x}_i\\cdot\\vec{y}_j$$\n",
    "\n",
    "There are NumPy functions to compute each of these terms:\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{y}_j$ $\\rightarrow$ `np.dot(x, y)` : Matrix product of $\\{\\vec{x}\\}$ and $\\{\\vec{y}\\}$\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{x}_i$ $\\rightarrow$ `np.einsum('ij,ij->i', x, x)[:,np.newaxis]` : A $(n,1)$ vector of elements $\\sum_j x_{ij}x_{ij}$\n",
    "\n",
    "$\\vec{y}_j\\cdot\\vec{y}_j$ $\\rightarrow$ `np.einsum('ij,ij->i', y, y)[:,np.newaxis].T` : A $(1,n)$ vector of elements $\\sum_j y_{ij}y_{ij}$\n",
    "\n",
    "To have all the combinations $ij$ of the sum $\\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j$, we add a new axis to each of the arrays, transpose one them and add them.\n",
    "\n",
    "Let's see now how the `np.einsum` function works. `einsum` stands for Einstein summation, which is used in tensor algebra to write compact expressions without the sum symbol ($\\sum$). Within the Einstein summation notation, whenever there are repeated indexes, there is a sum over them. For instance, the expression\n",
    "$$x_{ik}y_{kj}$$\n",
    "is equivalent to\n",
    "$$\\sum_k x_{ik}y_{kj}$$\n",
    "\n",
    "`np.einsum` uses a generalized form of the Einstein summation by adding the symbol `->` to prevent summing over certain indexes. The specific operation we use here, `np.einsum('ij,ij->i', x, x)`, gives the vector\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum_k x_{1k}x_{1k} \\\\\n",
    "\\sum_k x_{2k}x_{2k} \\\\\n",
    " ...                \\\\\n",
    "\\sum_k x_{nk}x_{nk} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that the resulting vector is represented here as a column vector just for visualization purposes. It's is `(n,)` NumPy array.\n",
    "\n",
    "Let's check now step-by-step what the `euclidean_numpy` function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets generate some random data\n",
    "nsamples = 10\n",
    "nfeat = 3\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x2 + x2.T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `np.dot` to perform the matrix multiplication of the full dataset by itself. We didn't use it before as alternative to `np.einsum` because it doesn't perform row by row scalar products. Instead `np.dot` expects two arrays with matching shapes $(m,n)$ and $(n,m)$ to perform a matrix multiplication.\n",
    "\n",
    "We could have used `np.einsum('ik,jk', x, x)` to perform the matrix multiplication, but we chose `np.dot(x, x.T)` instead. This is because `np.dot` is a very sophisticated too, plus it uses OpenMP threads. This results in a very fast execution.\n",
    "\n",
    "You are wellcome to time them and look at the `top` command to see how `np.dot` uses multiple OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = np.dot(x, x.T)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, considering that the reason we are using `np.einsum` is to get rid of the loops, why didn't we use something like `(x*x).sum(axis=1)`? Let's run the next cell comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 µs ± 126 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "244 µs ± 59.1 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# let's use a larger array for timing the function calls\n",
    "nsamples = 1000\n",
    "nfeat = 300\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "# it gives the same result\n",
    "np.abs(np.einsum('ij,ij->i', x, x) - (x*x).sum(axis=1)).max()\n",
    "\n",
    "# but it's not as fast as `np.einsum`\n",
    "%timeit np.einsum('ij,ij->i', x, x)\n",
    "%timeit (x*x).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a reduction with the ufunc `np.add` is also slower than `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 µs ± 97.9 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.add.reduce(x*x, axis=1)\n",
    "\n",
    "# As homework, check what's the meaning of the previous line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's time both implementations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.76 s ± 18.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "48.2 ms ± 25.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "nsamples = 2000\n",
    "nfeat = 50\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "%timeit euclidean_loops(x, x)\n",
    "%timeit euclidean_numpy(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main points to take from this notebook are:\n",
    "  * Loops in python must be avoided.\n",
    "  * NumPy is all about vectorization.\n",
    "  * Always consider different vectorized implementations and compare them. Even within NumPy some functions might bring a more significant speedup than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
